{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KveCdUAUuQZ"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ollama openai pandas tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsieoWS7a2fL"
   },
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!sudo apt-get install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rnYwg_ha6WW"
   },
   "outputs": [],
   "source": [
    "# run ollama server on Colab\n",
    "import os\n",
    "import threading\n",
    "import subprocess\n",
    "\n",
    "def start_ollama():\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "ollama_thread = threading.Thread(target=start_ollama)\n",
    "ollama_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdDq3FyPbNX9"
   },
   "outputs": [],
   "source": [
    "# Download LLM\n",
    "!ollama pull mannix/gemma2-9b-simpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NM0kNP5r6a-5"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import json\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate progress bare for pandas application\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOR_GQHP6pft"
   },
   "outputs": [],
   "source": [
    "# Improved codebook with more specific categories and examples\n",
    "categories_codebook = \"\"\"\n",
    "Climate Change Denial Arguments Codebook:\n",
    "- 1.1 Ice, permafrost, or snow cover isn't melting.\n",
    "- 1.2 We're heading into global cooling or a new ice age.\n",
    "- 1.3 Cold weather or snow means there's no global warming.\n",
    "- 1.4 The climate hasn't warmed or changed in recent decades.\n",
    "- 1.5 The oceans are cooling, or they're not warming.\n",
    "- 1.6 Sea level rise is exaggerated or isn't accelerating.\n",
    "- 1.7 Extreme weather isn't increasing, has always happened, or isn't linked to climate change.\n",
    "- 1.8 They changed the term from 'global warming' to 'climate change' because it's not really warming.\n",
    "- 2.1 Climate change is just part of natural cycles or variations.\n",
    "- 2.2 Human impacts other than greenhouse gases (like aerosols or land use) are the cause.\n",
    "- 2.3 There's no real evidence that CO2 or the greenhouse effect is driving climate change.\n",
    "- 2.4 CO2 levels aren't rising, or the ocean's pH isn't dropping.\n",
    "- 2.5 Human CO2 emissions are too small to make a difference.\n",
    "- 3.1 The climate isn't very sensitive to CO2, and there are feedbacks that reduce warming.\n",
    "- 3.2 Species, plants, or coral reefs aren't affected by climate change yet, or they are even benefiting.\n",
    "- 3.3 CO2 is good, not a pollutant.\n",
    "- 3.4 The temperature increase is only a few degrees, which isn't a big deal.\n",
    "- 3.5 Climate change doesn't contribute to human conflict or threaten national security.\n",
    "- 3.6 Climate change doesn't have negative effects on health.\n",
    "- 4.1 Climate policies, whether mitigation or adaptation, are harmful.\n",
    "- 4.2 Climate policies are ineffective or flawed.\n",
    "- 4.3 The problem is too hard to solve.\n",
    "- 4.4 Clean energy technologies or biofuels won't work.\n",
    "- 4.5 We need energy from fossil fuels or nuclear power.\n",
    "- 5.1 Climate science is uncertain, unsound, or unreliable (refers to data, methods, or models).\n",
    "- 5.2 The climate movement is alarmist, wrong, political, biased, or hypocritical.\n",
    "- 5.3 Climate change science or policy is a conspiracy or a deception.\n",
    "- 0.0 None of the above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmW6NZ8-6-Di"
   },
   "outputs": [],
   "source": [
    "# Main function\n",
    "def classify_claim(claim):\n",
    "   prompt = f\"\"\"\n",
    "   Given the following Climate Change Denial Arguments Codebook:\n",
    "   {categories_codebook}\n",
    "   Classify the following claim into one of the categories. Pick the one that fits best - if multiple, pick the most relevant one.\n",
    "   Claim: {claim}\n",
    "   Output only the category number as a float in JSON format, like this: {{\"category\": 1.1}}\n",
    "   \"\"\"\n",
    "   response = ollama.chat(\n",
    "       model='mannix/gemma2-9b-simpo:latest',\n",
    "       messages=[\n",
    "           {\"role\": \"system\", \"content\": \"You are a climate change claim classification assistant. Classify the given claim according to the codebook.\"},\n",
    "           {\"role\": \"user\", \"content\": prompt}\n",
    "       ],\n",
    "       format='json'\n",
    "   )\n",
    "   try:\n",
    "       result = json.loads(response['message']['content'])\n",
    "       return float(result['category'])\n",
    "   except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "       print(f\"Error parsing LLM response: {e}\")\n",
    "       print(f\"Full response: {response['message']['content']}\")\n",
    "       return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2qIQvsMHgTx"
   },
   "outputs": [],
   "source": [
    "\n",
    "def gwet_ac1(ratings1, ratings2):\n",
    "   \"\"\"Calculate Gwet's AC1\"\"\"\n",
    "   n = len(ratings1)\n",
    "   categories = sorted(set(ratings1) | set(ratings2))\n",
    "   q = len(categories)\n",
    "\n",
    "   # Calculate observed agreement\n",
    "   pa = sum(r1 == r2 for r1, r2 in zip(ratings1, ratings2)) / n\n",
    "\n",
    "   # Calculate chance agreement\n",
    "   pi = [(sum(r1 == cat for r1 in ratings1) +\n",
    "          sum(r2 == cat for r2 in ratings2)) / (2 * n)\n",
    "         for cat in categories]\n",
    "   peg = sum(p * (1 - p) for p in pi) / (q - 1)\n",
    "\n",
    "   # Calculate Gwet's AC1\n",
    "   ac1 = (pa - peg) / (1 - peg)\n",
    "   return ac1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EObXUDCKHnNz"
   },
   "outputs": [],
   "source": [
    "def test_randomness(codes):\n",
    "   \"\"\"Perform tests of randomness\"\"\"\n",
    "   unique_codes = sorted(set(codes))\n",
    "\n",
    "   if len(unique_codes) == 2:  # Binary case\n",
    "       count = sum(codes == unique_codes[1])\n",
    "       nobs = len(codes)\n",
    "       stat, pval = proportions_ztest(count, nobs, 0.5)\n",
    "       return pval\n",
    "   else:  # Multiple categories\n",
    "       observed = pd.Series(codes).value_counts()\n",
    "       expected = np.ones(len(unique_codes)) * len(codes) / len(unique_codes)\n",
    "       stat, pval = chi2_contingency([observed, expected])[0:2]\n",
    "       return pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDLo2CMRH4wy"
   },
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/aaubs/llm-content-analysis/main/data/contrarian_claims_reasons.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3RiYhCiH7BX"
   },
   "outputs": [],
   "source": [
    "# Apply the classification function to the 'text' column with tqdm\n",
    "df['new_model_code'] = df['text'].progress_apply(classify_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umIIaD86H8Bg"
   },
   "outputs": [],
   "source": [
    "# Convert codes to float\n",
    "df['original_code'] = df['original_code'].astype(float)\n",
    "df['replicated_code'] = df['replicated_code'].astype(float)\n",
    "df['model_code'] = df['model_code'].astype(float)\n",
    "df['new_model_code'] = df['new_model_code'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Si_JJEyhH_0c"
   },
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "results = {\n",
    "   'human_human_ac1': gwet_ac1(df['original_code'], df['replicated_code']),\n",
    "   'human_model_ac1': gwet_ac1(df['original_code'], df['model_code']),\n",
    "   'human_newmodel_ac1': gwet_ac1(df['original_code'], df['new_model_code']),\n",
    "   'model_newmodel_ac1': gwet_ac1(df['model_code'], df['new_model_code']),\n",
    "   'randomness_pval_original': test_randomness(df['model_code']),\n",
    "   'randomness_pval_new': test_randomness(df['new_model_code'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwyqlPIqM10H"
   },
   "source": [
    "The interpretation of Gwet’s AC1 values is similar to other agreement statistics like Cohen’s kappa, and the “goodness” of the values depends on the context. Here’s a general guide for interpreting Gwet’s AC1:\n",
    "\n",
    "General Interpretation:\n",
    "\n",
    "\t•\t0.81 to 1.00: Almost perfect agreement\n",
    "\t•\t0.61 to 0.80: Substantial agreement\n",
    "\t•\t0.41 to 0.60: Moderate agreement\n",
    "\t•\t0.21 to 0.40: Fair agreement\n",
    "\t•\t0.00 to 0.20: Slight agreement\n",
    "\t•\tBelow 0.00: Poor or no agreement (worse than chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrQWvt2DICuX"
   },
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Agreement Metrics (Gwet's AC1):\")\n",
    "print(f\"Human-Human: {results['human_human_ac1']:.3f}\")\n",
    "print(f\"Human-Original Model: {results['human_model_ac1']:.3f}\")\n",
    "print(f\"Human-New Model: {results['human_newmodel_ac1']:.3f}\")\n",
    "print(f\"Model-Model: {results['model_newmodel_ac1']:.3f}\")\n",
    "print(\"\\nRandomness Test p-values:\")\n",
    "print(f\"Original Model: {results['randomness_pval_original']:.3f}\")\n",
    "print(f\"New Model: {results['randomness_pval_new']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoiN_AcxID3U"
   },
   "outputs": [],
   "source": [
    "# Convert float codes to string labels for confusion matrix\n",
    "df['model_code_str'] = df['model_code'].astype(str)\n",
    "df['new_model_code_str'] = df['new_model_code'].astype(str)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(df['model_code_str'], df['new_model_code_str'])\n",
    "\n",
    "# Get actual labels from confusion matrix\n",
    "actual_labels = list(range(conf_matrix.shape[0]))\n",
    "\n",
    "conf_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=[f'True_{label}' for label in actual_labels],\n",
    "    columns=[f'Pred_{label}' for label in actual_labels]\n",
    ")\n",
    "\n",
    "# Add row/column totals\n",
    "conf_df['Total'] = conf_df.sum(axis=1)\n",
    "conf_df.loc['Total'] = conf_df.sum()\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qyn0DX1ZIFS2"
   },
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nClassification Report (New Model vs Original Model):\")\n",
    "print(classification_report(df['model_code_str'], df['new_model_code_str']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IoaU3n6KG7o"
   },
   "source": [
    "## TogetherAI (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HfiFhcnKrRZ"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNmGJiBWKOqk"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsWJhzeRKt-j"
   },
   "outputs": [],
   "source": [
    "# Setup OpenAI client with custom API key and base URL\n",
    "TOGETHER_API_KEY = userdata.get('TOGETHER_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=TOGETHER_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTjxOBvMGAMl"
   },
   "outputs": [],
   "source": [
    "def classify_claim_openai(claim):\n",
    "   prompt = f\"\"\"Given the following Climate Change Denial Arguments Codebook:\n",
    "{categories_codebook}\n",
    "Classify the following claim into one of the categories. Pick the one that fits best - if multiple, pick the most relevant one.\n",
    "Claim: {claim}\n",
    "Output only the category number as a float in JSON format, like this: {{\"category\": 1.1}}\"\"\"\n",
    "\n",
    "   response = client.chat.completions.create(\n",
    "       model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "       messages=[\n",
    "           {\"role\": \"system\", \"content\": \"You are a climate change claim classification assistant. Classify the given claim according to the codebook.\"},\n",
    "           {\"role\": \"user\", \"content\": prompt}\n",
    "       ],\n",
    "       temperature=0,\n",
    "       response_format={\"type\": \"json_object\"}\n",
    "   )\n",
    "   try:\n",
    "       result = json.loads(response.choices[0].message.content)\n",
    "       return float(result['category'])\n",
    "   except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "       print(f\"Error parsing response: {e}\")\n",
    "       print(f\"Full response: {response.choices[0].message.content}\")\n",
    "       return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzPqA7cmMEcV"
   },
   "outputs": [],
   "source": [
    "# Add new column for OpenAI model predictions\n",
    "df['openai_model_code'] = df['text'].progress_apply(classify_claim_openai)\n",
    "df['openai_model_code'] = df['openai_model_code'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fpm7rm_QMA_L"
   },
   "outputs": [],
   "source": [
    "# Calculate metrics including OpenAI model\n",
    "results = {\n",
    "   'human_human_ac1': gwet_ac1(df['original_code'], df['replicated_code']),\n",
    "   'human_model_ac1': gwet_ac1(df['original_code'], df['model_code']),\n",
    "   'human_gemma_ac1': gwet_ac1(df['original_code'], df['new_model_code']),\n",
    "   'human_openai_ac1': gwet_ac1(df['original_code'], df['openai_model_code']),\n",
    "   'model_gemma_ac1': gwet_ac1(df['model_code'], df['new_model_code']),\n",
    "   'model_openai_ac1': gwet_ac1(df['model_code'], df['openai_model_code']),\n",
    "   'gemma_openai_ac1': gwet_ac1(df['new_model_code'], df['openai_model_code'])\n",
    "}\n",
    "\n",
    "print(\"\\nAgreement Metrics (Gwet's AC1):\")\n",
    "for k, v in results.items():\n",
    "   print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp7Fx_FfNNeC"
   },
   "outputs": [],
   "source": [
    "# Confusion matrices between all model pairs\n",
    "model_pairs = [\n",
    "   ('model_code', 'new_model_code', 'Original-Gemma'),\n",
    "   ('model_code', 'openai_model_code', 'Original-OpenAI'),\n",
    "   ('new_model_code', 'openai_model_code', 'Gemma-OpenAI')\n",
    "]\n",
    "\n",
    "for col1, col2, name in model_pairs:\n",
    "   conf = confusion_matrix(df[col1].astype(str), df[col2].astype(str))\n",
    "   conf_df = pd.DataFrame(conf)\n",
    "   print(f\"\\nConfusion Matrix {name}:\")\n",
    "   print(conf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9KQZLCiNbJo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
